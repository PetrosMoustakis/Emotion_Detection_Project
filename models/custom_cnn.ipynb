{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# IMPORT LIBRARIES\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization, GlobalAveragePooling2D, LeakyReLU, SpatialDropout2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from src.config import image_classes\n",
    "\n",
    "print(\"âœ… Libraries installed and imported.\")"
   ],
   "id": "1530e007377e08c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data = np.load('/emotion_detection_data.npz')",
   "id": "e0c631e68dbb8214"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x_train = data['x_train']\n",
    "y_train = data['y_train']\n",
    "x_val = data['x_val']\n",
    "y_val = data['y_val']\n",
    "x_test = data['x_test']\n",
    "y_test = data['y_test']"
   ],
   "id": "f50723a6249ab57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluation_function(history, model):\n",
    "    \"\"\"\n",
    "    Evaluates a trained Keras model by plotting training history, calculating\n",
    "    accuracy on train, validation, and test sets, and visualizing the confusion matrix.\n",
    "\n",
    "    This function expects `x_test`, `y_test`, `x_train`, `y_train`, `x_val`,\n",
    "    `y_val`, and `image_classes` to be globally accessible or passed implicitly\n",
    "    from the surrounding scope.\n",
    "\n",
    "    Args:\n",
    "        history (keras.callbacks.History): A History object returned by the `fit`\n",
    "            method of a Keras model. It contains the loss and metrics values\n",
    "            during training.\n",
    "        model (keras.Model or str): The Keras model object itself, or the path\n",
    "            to the saved weights file of the best performing model. The function\n",
    "            will load weights into `history.model` if `model` is a path.\n",
    "\n",
    "    Plots:\n",
    "        - **Model Loss:** A plot showing training loss and validation loss over epochs.\n",
    "        - **Model Accuracy:** A plot showing training accuracy and validation accuracy over epochs.\n",
    "        - **Confusion Matrix:** A heatmap visualizing the confusion matrix for\n",
    "          predictions on the test set.\n",
    "\n",
    "    Prints:\n",
    "        - The test accuracy of the submitted model.\n",
    "        - The train accuracy of the submitted model.\n",
    "        - The validation accuracy of the submitted model.\n",
    "    \"\"\"\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    history.model.load_weights(model)\n",
    "    test_acc = np.mean(np.argmax(history.model.predict(x_test),1)==np.argmax(y_test,1))\n",
    "    print('The submitted model has test accuracy equal to {}'.format(test_acc))\n",
    "\n",
    "    train_acc = np.mean(np.argmax(history.model.predict(x_train),1)==np.argmax(y_train,1))\n",
    "    print('The submitted model has train accuracy equal to {}'.format(train_acc))\n",
    "\n",
    "    val_acc = np.mean(np.argmax(history.model.predict(x_val),1)==np.argmax(y_val,1))\n",
    "    print('The submitted model has validation accuracy equal to {}'.format(val_acc))\n",
    "\n",
    "    y_pred_probs = model.predict(x_test)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "    # Confusion matrix visualization\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=image_classes, yticklabels=image_classes)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()"
   ],
   "id": "2277bf7645866356"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "classes = 7\n",
    "batch = 16\n",
    "optimizer = AdamW(learning_rate=0.0001, weight_decay=0.0001)\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train.argmax(axis=1)), y=y_train.argmax(axis=1))\n",
    "class_weights = dict(enumerate(class_weights))"
   ],
   "id": "b1502613be371c55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1st custom cnn Model\n",
    "\n",
    "model_1 = Sequential([\n",
    "\n",
    "    Conv2D(32, (3,3), padding='same', strides=(1,1), activation='relu', input_shape=(48,48,1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D((2,2)),\n",
    "    SpatialDropout2D(0.2),\n",
    "\n",
    "    Conv2D(64, (3,3), padding='same', strides=(1,1), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D((2,2)),\n",
    "    SpatialDropout2D(0.2),\n",
    "\n",
    "    Conv2D(128, (3,3), padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D((2,2)),\n",
    "    SpatialDropout2D(0.3),\n",
    "\n",
    "    Conv2D(256, (3,3), padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D((2,2)),\n",
    "    SpatialDropout2D(0.3),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(7, activation='softmax')\n",
    "])"
   ],
   "id": "ea62149f5aa70e0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "epochs = 50\n",
    "\n",
    "save_model = ModelCheckpoint('custom_cnn_1.keras', save_best_only=True, monitor='val_loss', mode='min', verbose=1)\n",
    "early_stop = EarlyStopping(monitor='val_accuracy',  patience=10,  restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience=5, min_lr=0.00001, verbose=1)\n",
    "model_1.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "history = model_1.fit(x_train, y_train, batch_size=batch, epochs=epochs, validation_data=(x_val, y_val), shuffle = True, verbose=1,\n",
    "                    callbacks=[save_model, early_stop, lr_scheduler], class_weight = class_weights)\n",
    "\n",
    "evaluation_function(history, 'custom_cnn_1.keras')"
   ],
   "id": "1d3a2a23d99e8d68"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_2 = Sequential([\n",
    "\n",
    "    Conv2D(32, (3,3), padding='same', strides=(1,1), activation='linear', input_shape=(48,48,1)),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(32, (3,3), padding='same', strides=(1,1), activation='linear'),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D((2,2)),\n",
    "    SpatialDropout2D(0.20),\n",
    "\n",
    "    Conv2D(64, (3,3), padding='same', strides=(1,1), activation='linear'),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, (3,3), padding='same', strides=(1,1), activation='linear'),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D((2,2)),\n",
    "    SpatialDropout2D(0.25),\n",
    "\n",
    "    Conv2D(128, (3,3), padding='same', activation='linear'),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(128, (3,3), padding='same', activation='linear'),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D((2,2)),\n",
    "    SpatialDropout2D(0.3),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(256, activation='linear'),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(256, activation='linear'),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(7, activation='softmax')\n",
    "])\n",
    "\n",
    "print(model_2.summary())"
   ],
   "id": "2f8bc8aedacb9ed1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "epochs = 60\n",
    "\n",
    "save_model = ModelCheckpoint('custom_cnn_2.keras', save_best_only=True, monitor='val_loss', mode='min', verbose=2)\n",
    "model_2.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "history_2 = model_2.fit(x_train, y_train, batch_size=batch, epochs=epochs, validation_data=(x_val, y_val), shuffle = True, verbose=1,\n",
    "                    callbacks=[save_model, early_stop, lr_scheduler], class_weight = class_weights)\n",
    "\n",
    "evaluation_function(history_2, 'custom_cnn_2.keras')"
   ],
   "id": "60b7997043afab3e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_3 = Sequential([\n",
    "\n",
    "    Conv2D(32, (3,3), padding='same', activation='relu', kernel_initializer='he_normal', input_shape=(48,48,1)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(32, (3,3), padding='same', kernel_initializer='he_normal', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(32, (3,3), padding='same', kernel_initializer='he_normal', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D((2,2), strides=(2,2)),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Conv2D(64, (3,3), padding='same', kernel_initializer='he_normal', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, (3,3), padding='same', kernel_initializer='he_normal', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, (3,3), padding='same', kernel_initializer='he_normal', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D((2,2), strides=(2,2)),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Conv2D(128, (3,3), padding='same',kernel_initializer='he_normal', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(128, (3,3), padding='same', kernel_initializer='he_normal', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D((2,2), strides=(2,2)),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Conv2D(256, (3,3), padding='same', kernel_initializer='he_normal', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(256, (3,3), padding='same', kernel_initializer='he_normal', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D((2,2), strides=(2,2)),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(512, activation='linear'),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(7, activation='softmax')\n",
    "])\n",
    "\n",
    "print(model_3.summary())"
   ],
   "id": "fe97f6f5cd128af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "epochs = 30\n",
    "\n",
    "save_model = ModelCheckpoint('custom_cnn_3.keras', save_best_only=True, monitor='val_loss', mode='min', verbose=2)\n",
    "model_3.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "history_3 = model_3.fit(x_train, y_train, batch_size=batch, epochs=epochs, validation_data=(x_val, y_val), shuffle = True, verbose=1,\n",
    "                    callbacks=[save_model, early_stop, lr_scheduler], class_weight = class_weights)\n",
    "\n",
    "evaluation_function(history_3, 'custom_cnn_3.keras')"
   ],
   "id": "e5c138e391e734b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_4 = Sequential([\n",
    "\n",
    "    Conv2D(64, (3,3), padding='same',kernel_initializer='he_normal', activation='relu', input_shape=(48,48,1)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, (3,3), padding='same',kernel_initializer='he_normal', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D((2,2), strides=(2,2)),\n",
    "\n",
    "    Conv2D(128, (3,3), padding='same',kernel_initializer='he_normal', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(128, (3,3), padding='same',kernel_initializer='he_normal', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D((2,2), strides=(2,2)),\n",
    "\n",
    "    Conv2D(256, (3,3), padding='same',kernel_initializer='he_normal', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(256, (3,3), padding='same',kernel_initializer='he_normal', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(256, (3,3), padding='same',kernel_initializer='he_normal', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D((2,2), strides=(2,2)),\n",
    "    SpatialDropout2D(0.2),\n",
    "\n",
    "    Conv2D(512, (3,3), padding='same',kernel_initializer='he_normal', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(512, (3,3), padding='same',kernel_initializer='he_normal', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(512, (3,3), padding='same',kernel_initializer='he_normal', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D((2,2), strides=(2,2)),\n",
    "    SpatialDropout2D(0.2),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dense(7, activation='softmax')\n",
    "])\n",
    "\n",
    "print(model_4.summary())"
   ],
   "id": "3ac371e57e1b58ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "epochs = 25\n",
    "\n",
    "save_model = ModelCheckpoint('custom_cnn_4.keras', save_best_only=True, monitor='val_loss', mode='min', verbose=2)\n",
    "model_4.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "history = model_4.fit(x_train, y_train, batch_size=batch, epochs=epochs, validation_data=(x_val, y_val), shuffle = True, verbose=1,\n",
    "                    callbacks=[save_model, early_stop, lr_scheduler], class_weight = class_weights)\n",
    "\n",
    "evaluation_function(history_3, 'custom_cnn_4.keras')"
   ],
   "id": "8647ba2307c53648"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
