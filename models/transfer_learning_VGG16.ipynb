{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. IMPORT LIBRARIES\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "from src.config import train_dir, test_dir, val_dir\n",
    "print(\"✅ Libraries installed and imported.\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "transfer_img_height = 224\n",
    "transfer_img_width = 224\n",
    "transfer_color_mode = 'rgb'\n",
    "batch_size = 32"
   ],
   "id": "f8f7780d12bdf172"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    labels='inferred',\n",
    "    label_mode = 'categorical',\n",
    "    image_size = (transfer_img_height, transfer_img_width),\n",
    "    color_mode = transfer_color_mode,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    seed = 42\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    val_dir,\n",
    "    labels='inferred',\n",
    "    label_mode = 'categorical',\n",
    "    image_size = (transfer_img_height, transfer_img_width),\n",
    "    color_mode = transfer_color_mode,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    labels='inferred',\n",
    "    label_mode = 'categorical',\n",
    "    image_size = (transfer_img_height, transfer_img_width),\n",
    "    color_mode = transfer_color_mode,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False\n",
    ")\n",
    "\n",
    "preprocess_input_vgg = tf.keras.applications.vgg16.preprocess_input\n",
    "\n",
    "train_ds_final = train_ds.map(lambda x, y: (preprocess_input_vgg(x), y)).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_ds_final = val_ds.map(lambda x, y: (preprocess_input_vgg(x), y)).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_ds_final = test_ds.map(lambda x, y: (preprocess_input_vgg(x), y)).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "print(\"✅ Datasets for transfer learning are ready.\")"
   ],
   "id": "c7d59ac939fd39e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def calculate_weights_from_dir(directory):\n",
    "    \"\"\"\n",
    "    Calculates class weights by counting files in subdirectories.\n",
    "    This is much faster than iterating through a tf.data.Dataset.\n",
    "\n",
    "    Args:\n",
    "        directory (str): The path to the directory containing class subfolders\n",
    "                         (e.g., your TRAIN_DIR).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping class index to its calculated weight.\n",
    "    \"\"\"\n",
    "    print(f\"Calculating class weights from directory: {directory}\")\n",
    "\n",
    "    # Get class names from subfolder names, sorted alphabetically\n",
    "    # This ensures consistent ordering with what image_dataset_from_directory does\n",
    "    class_names = sorted([d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))])\n",
    "\n",
    "    num_classes = len(class_names)\n",
    "    class_counts = []\n",
    "\n",
    "    # Count files in each class subfolder\n",
    "    for class_name in class_names:\n",
    "        class_path = os.path.join(directory, class_name)\n",
    "        count = len(os.listdir(class_path))\n",
    "        class_counts.append(count)\n",
    "        print(f\"- Found {count} images for class '{class_name}'\")\n",
    "\n",
    "    class_counts = np.array(class_counts)\n",
    "    total_samples = np.sum(class_counts)\n",
    "\n",
    "    # Calculate weights using the 'balanced' formula:\n",
    "    # weight = total_samples / (num_classes * count_for_that_class)\n",
    "    class_weights_array = total_samples / (num_classes * class_counts)\n",
    "\n",
    "    # Create the dictionary mapping class index to weight\n",
    "    class_weights_dict = {i: weight for i, weight in enumerate(class_weights_array)}\n",
    "\n",
    "    print(\"\\nCalculated Class Weights:\", class_weights_dict)\n",
    "    return class_weights_dict"
   ],
   "id": "b540fa6eeab1157a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class_weights = calculate_weights_from_dir(train_dir)\n",
    "\n",
    "print(class_weights)"
   ],
   "id": "5c83bb3097c33998"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate_models(history, model):\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    best_model = tf.keras.models.load_model(model)\n",
    "    # Evaluation\n",
    "    print(\"\\nEvaluating model performance on the train set...\\n\")\n",
    "    results = best_model.evaluate(train_ds_final, verbose=1)\n",
    "\n",
    "    train_loss = results[0]\n",
    "    train_accuracy = results[1]\n",
    "\n",
    "    print(f\"Final Train Loss: {train_loss:.4f}\\n\")\n",
    "    print(f\"Final Train Accuracy: {train_accuracy * 100:.2f}%\\n\")\n",
    "\n",
    "    print(\"\\nEvaluating model performance on the val set...\\n\")\n",
    "    results = best_model.evaluate(val_ds_final, verbose=1)\n",
    "\n",
    "    val_loss = results[0]\n",
    "    val_accuracy = results[1]\n",
    "\n",
    "    print(f\"Final Val Loss: {val_loss:.4f}\\n\")\n",
    "    print(f\"Final Val Accuracy: {val_accuracy * 100:.2f}%\\n\")\n",
    "\n",
    "    print(\"\\nEvaluating model performance on the test set...\\n\")\n",
    "    results = best_model.evaluate(test_ds_final, verbose=1)\n",
    "\n",
    "    test_loss = results[0]\n",
    "    test_accuracy = results[1]\n",
    "\n",
    "    print(f\"Final Test Loss: {test_loss:.4f}\\n\")\n",
    "    print(f\"Final Test Accuracy: {test_accuracy * 100:.2f}%\\n\")\n",
    "\n",
    "    y_pred_probs  = best_model.predict(test_ds_final)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    y_true = []\n",
    "    for images, labels in test_ds_final:\n",
    "      y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
    "\n",
    "      image_classes = train_ds.class_names\n",
    "\n",
    "    # Confusion matrix visualization\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=image_classes, yticklabels=image_classes)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\n\\nClassification Report:\\n{classification_report(y_true, y_pred, target_names=image_classes)}\")"
   ],
   "id": "bf0c656f42bab517"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_classes = 7\n",
    "input_shape = (transfer_img_height, transfer_img_width, 3)\n",
    "\n",
    "batch = 32\n",
    "epochs = 50\n",
    "optimizer = AdamW(learning_rate=0.001, weight_decay=0.0001)"
   ],
   "id": "62498422b6fe9e22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def build_transfer_model(input_shape, num_classes):\n",
    "  base_model = tf.keras.applications.VGG16(\n",
    "      include_top = False,\n",
    "      weights = 'imagenet',\n",
    "      input_shape = input_shape\n",
    "  )\n",
    "\n",
    "  base_model.trainable = False\n",
    "\n",
    "  model = Sequential([\n",
    "      base_model,\n",
    "      Flatten(),\n",
    "      Dense(256, activation='relu'),\n",
    "      Dropout(0.5),\n",
    "      Dense(num_classes, activation='softmax')\n",
    "  ], name=\"VGG16_Transfer_Model\")\n",
    "\n",
    "  return model"
   ],
   "id": "9c13da7f8aa25b78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "transfer_model = build_transfer_model(input_shape=input_shape, num_classes=num_classes)",
   "id": "3900ca86bcff6867"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "save_model = ModelCheckpoint('vgg16_model.keras', save_best_only=True, monitor='val_loss', mode='min', verbose=2)\n",
    "early_stop = EarlyStopping(monitor='val_accuracy',  patience=10,  restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
    "transfer_model.compile(\n",
    "    optimizer = AdamW(learning_rate=0.0001, weight_decay=0.0001),\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "history = transfer_model.fit(train_ds_final, epochs=epochs, validation_data=val_ds_final,\n",
    "                    callbacks=[save_model, early_stop, lr_scheduler], class_weight=class_weights, verbose=1)\n",
    "\n",
    "evaluate_models(history, model='vgg16_model.keras')"
   ],
   "id": "4da7a8f2ba8a350"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
